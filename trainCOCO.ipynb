{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:37.013669Z",
     "start_time": "2018-02-19T04:32:37.008071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%set_env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:43.186357Z",
     "start_time": "2018-02-19T04:32:37.015680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "#from data import v2, v1, AnnotationTransform, VOCDetection, detection_collate, VOCroot, VOC_CLASSES\n",
    "from data import v2, v1, detection_collate\n",
    "from data import MSCOCODetection, COCOAnnotationTransform, COMMON_CLASSES\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from logger import Logger\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#hyperparameters\n",
    "\n",
    "version = \"v2\"\n",
    "basenet = \"vgg16_reducedfc.pth\"\n",
    "jaccard_threshold=0.5\n",
    "lr=8e-4\n",
    "\n",
    "save_folder=\"./MSCOCO14weights/\"\n",
    "\n",
    "voc_root= \"/new_data/gpu/utkrsh/coco/\" # location of the image root directory\n",
    "\n",
    "annFile = \"/new_data/gpu/utkrsh/coco/annotations/instances_train2014.json\"\n",
    "train_img = \"/new_data/gpu/utkrsh/coco/images/train2014/\"\n",
    "\n",
    "cuda=True\n",
    "# resume = \"./MSCOCO14weights/ssd300_COCO_8000.pth\" # saved trained weights\n",
    "resume = None\n",
    "start_iter = 1\n",
    "\n",
    "if cuda and torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "cfg = (v1, v2)[version == 'v2']\n",
    "\n",
    "#train_sets = [('2007', 'trainval'), ('2012', 'trainval')]\n",
    "# train_sets = 'train'\n",
    "ssd_dim = 300  # only support 300 now\n",
    "means = (104, 117, 123)  # imagenet mean values\n",
    "num_classes = len(COMMON_CLASSES) + 1\n",
    "batch_size = 32\n",
    "accum_batch_size = 32\n",
    "iter_size = accum_batch_size / batch_size\n",
    "max_iter = 50000\n",
    "weight_decay = 0.0005\n",
    "stepvalues = (4000, 80000, 120000, 140000)\n",
    "lr_steps = { 4000 : 0.001,\n",
    "           80000 : 0.0001,\n",
    "           120000 : 0.00001\n",
    "           }\n",
    "gamma = 0.1\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:58.760285Z",
     "start_time": "2018-02-19T04:32:43.189741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n"
     ]
    }
   ],
   "source": [
    "ssd_net = build_ssd('train', 300, num_classes)\n",
    "net = ssd_net\n",
    "\n",
    "\n",
    "if resume:\n",
    "    print(\"Resuming training, loading weights from {}...\".format(resume))\n",
    "    ssd_net.load_weights(resume)\n",
    "else:\n",
    "    vgg_weights = torch.load(save_folder + basenet)\n",
    "    print('Loading base network...')\n",
    "    ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "#     net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "if not resume:\n",
    "    print('Initializing weights...')\n",
    "    # initialize newly added layers' weights with xavier method\n",
    "    ssd_net.extras.apply(weights_init)\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss(num_classes, 0.5, True, 0, True, 3, 0.5, False, cuda)\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    #local_lr = lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_steps[step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:58.786872Z",
     "start_time": "2018-02-19T04:32:58.761746Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = Logger(\".logs/onlyCOCO14_baseline_model/\")\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:58.994046Z",
     "start_time": "2018-02-19T04:32:58.788479Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets_1 = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        # each sample is the result of one query on the dataset object\n",
    "        imgs.append(sample[0])\n",
    "        targets_1.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:59.221983Z",
     "start_time": "2018-02-19T04:32:58.997154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_tf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:32:59.497973Z",
     "start_time": "2018-02-19T04:32:59.229094Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    # loss counters\n",
    "    loc_loss = 0  # epoch\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print_interval = 5\n",
    "    curr_skip = 0\n",
    "    max_skip = 2\n",
    "    save_weights = True\n",
    "    st_values = lr_steps.keys()\n",
    "    \n",
    "    print('Loading Dataset...')\n",
    "    \n",
    "    dataset = MSCOCODetection(image_root=train_img, ann_root=annFile, \n",
    "                              transform=SSDAugmentation(ssd_dim, means), \n",
    "                              target_transform=COCOAnnotationTransform())\n",
    "    #dataset = MSCOCODetection(image_root=train_img, ann_root=annFile)\n",
    "    print(\"Dataset Loaded!\")\n",
    "    \n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print(\"Training SSD on\", dataset.name)\n",
    "    step_index = 0\n",
    "    batch_iterator = None\n",
    "    data_loader = data.DataLoader(dataset, batch_size, num_workers=4,\n",
    "                             shuffle=True, collate_fn=my_detection_collate,\n",
    "                             pin_memory=cuda)\n",
    "    \n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        if(not batch_iterator) or (iteration % epoch_size == 0):\n",
    "            # create batch_iterator\n",
    "            batch_iterator = iter(data_loader)\n",
    "        if iteration in st_values:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, gamma, iteration)\n",
    "            # reset epoch loss counters\n",
    "            epoch += 1\n",
    "        \n",
    "        images, targets = next(batch_iterator)\n",
    "        if cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(anno.cuda(), volatile=True) for anno in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(anno, volatile=True) for anno in targets]\n",
    "        \n",
    "        # forward pass\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if log_tf:\n",
    "            info = {\n",
    "                'loc_loss' : loss_l.data[0],\n",
    "                'conf_loss' : loss_c.data[0],\n",
    "                'loss' : loss.data[0]\n",
    "            }\n",
    "            for k, val in info.items():\n",
    "                logger.scalar_summary(k,val,iteration)\n",
    "            for k, val in net.named_parameters():\n",
    "                k = k.replace('.','/')\n",
    "                logger.histo_summary(k, to_np(val), iteration)\n",
    "                logger.histo_summary(k+'/grad', to_np(val.grad), iteration)\n",
    "        t1 = time.time()\n",
    "        if iteration % print_interval == 0:\n",
    "            print(\"Timer: %.4f sec. \" % (t1 - t0))\n",
    "            print(\"iter: \"+ repr(iteration) + \"|| loss_loc: %.4f || loss_conf: %.4f || loss: %.4f || \" \n",
    "                  % (loss_l.data[0], loss_c.data[0], loss.data[0]), end=' ')\n",
    "        if iteration % 5000 == 0 and save_weights:\n",
    "            try:\n",
    "                print(\" Saving state, iter: \", iteration)\n",
    "                torch.save(ssd_net.state_dict(), \"./MSCOCO14weights/ssd300_COCO_\" +\n",
    "                          repr(iteration) + \".pth\")\n",
    "            except:\n",
    "                print(\"cannot save the file\")\n",
    "    torch.save(ssd_net.state_dict(), \"./MSCOCO14weights/ssd300_COCO_final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:35:38.059004Z",
     "start_time": "2018-02-19T04:32:59.501465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=9.70s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset Loaded!\n",
      "Training SSD on COCO2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py:450: UserWarning: mask is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return tensor.masked_fill_(mask, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer: 1.6882 sec. \n",
      "iter: 5|| loss_loc: 3.6722 || loss_conf: 14.7035 || loss: 18.3757 ||  Timer: 1.6504 sec. \n",
      "iter: 10|| loss_loc: 3.1133 || loss_conf: 13.2061 || loss: 16.3195 ||  Timer: 1.6303 sec. \n",
      "iter: 15|| loss_loc: 3.0033 || loss_conf: 12.3313 || loss: 15.3345 ||  Timer: 1.7098 sec. \n",
      "iter: 20|| loss_loc: 3.5523 || loss_conf: 12.8119 || loss: 16.3642 ||  Timer: 1.6429 sec. \n",
      "iter: 25|| loss_loc: 3.5475 || loss_conf: 12.6853 || loss: 16.2328 ||  Timer: 1.6204 sec. \n",
      "iter: 30|| loss_loc: 4.2634 || loss_conf: 12.5221 || loss: 16.7855 ||  Timer: 1.6896 sec. \n",
      "iter: 35|| loss_loc: 3.3403 || loss_conf: 12.5912 || loss: 15.9315 ||  Timer: 1.6920 sec. \n",
      "iter: 40|| loss_loc: 3.4605 || loss_conf: 12.3936 || loss: 15.8541 ||  Timer: 1.6551 sec. \n",
      "iter: 45|| loss_loc: 3.6786 || loss_conf: 9.9091 || loss: 13.5877 ||  Timer: 1.6812 sec. \n",
      "iter: 50|| loss_loc: 3.2842 || loss_conf: 9.4374 || loss: 12.7216 ||  Timer: 1.6979 sec. \n",
      "iter: 55|| loss_loc: 4.3822 || loss_conf: 11.4375 || loss: 15.8197 ||  Timer: 1.7702 sec. \n",
      "iter: 60|| loss_loc: 3.7517 || loss_conf: 12.8925 || loss: 16.6442 ||  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "KeyboardInterrupt\n",
      "Process Process-3:\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/data/coco2014.py\", line 81, in __getitem__\n",
      "    im, gt, _, _ = self.pull_item(index)\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/data/coco2014.py\", line 111, in pull_item\n",
      "    img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/utils/augmentations.py\", line 417, in __call__\n",
      "    return self.augment(img, boxes, labels)\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/utils/augmentations.py\", line 52, in __call__\n",
      "    img, boxes, labels = t(img, boxes, labels)\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/utils/augmentations.py\", line 396, in __call__\n",
      "    im, boxes, labels = distort(im, boxes, labels)\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/utils/augmentations.py\", line 52, in __call__\n",
      "    img, boxes, labels = t(img, boxes, labels)\n",
      "  File \"/new_data/gpu/utkrsh/DomainAdaptation/utils/augmentations.py\", line 162, in __call__\n",
      "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/users/gpu/utkrsh/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e26945e00790>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             info = {\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;34m'loc_loss'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;34m'conf_loss'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;34m'loss'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "d_iter = iter(data_loader)\n",
    "d_img, d_targets = next(d_iter)\n",
    "if cuda:\n",
    "    d_img = Variable(d_img.cuda())\n",
    "else:\n",
    "    d_img = Variable(d_img)\n",
    "d_out = net(d_img)\n",
    "if cuda:\n",
    "    d_targets = [Variable(anno.cuda(), volatile=True) for anno in d_targets]\n",
    "else:\n",
    "    d_targets = [Variable(anno, volatile=True) for anno in d_targets]\n",
    "\n",
    "print(d_targets)\n",
    "\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    d_loss1, d_loss2 = criterion(d_out, d_targets)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
